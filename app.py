import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# =======================================================================
# NOVOS IMPORTS: O PADR√ÉO LANGCHAIN V1.0
# =======================================================================
from langchain_core.tools import tool, create_retriever_tool
# üö® A grande mudan√ßa est√° aqui: Usamos o novo 'create_agent'
from langchain.agents import create_agent

load_dotenv()

#1. Configura o "C√©rebro"
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)

#2. Conecta ao "Conhecimento" (Banco Vetorial)
modelo_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db = Chroma(persist_directory="BANCO_VETORIAL", embedding_function=modelo_embeddings)
retriever = vector_db.as_retriever(search_kwargs={"k": 5})

# =======================================================================
# CRIA√á√ÉO DAS FERRAMENTAS (OS "BRA√áOS" DA IA)
# =======================================================================

#FERRAMENTA 1: A Lupa
ferramenta_busca = create_retriever_tool(
    retriever,
    name="buscar_dados_transparencia",
    description="Busca informa√ß√µes financeiras e gastos p√∫blicos no banco de dados. Use esta ferramenta ANTES de responder qualquer pergunta sobre os dados."
)

# FERRAMENTA 2: A Impressora
@tool
def guardar_relatorio(nome_arquivo: str, conteudo: str) -> str:
    """
    Salva um relat√≥rio no disco local do computador.
    Par√¢metros:
    - nome_arquivo: O nome do arquivo a ser salvo (ex: fraude_cafe.txt). IMPORTANTE: inclua a extens√£o .txt.
    - conteudo: O texto completo do relat√≥rio detalhando o problema.
    """
    pasta_segura = "relatorios"
    caminho_final = os.path.join(pasta_segura, nome_arquivo)
    
    os.makedirs(pasta_segura, exist_ok=True)
    
    with open(caminho_final, 'w', encoding='utf-8') as ficheiro:
        ficheiro.write(conteudo)
        
    return f"SUCESSO: Arquivo {nome_arquivo} foi salvo fisicamente na pasta {pasta_segura}!"

ferramentas = [ferramenta_busca, guardar_relatorio]

# =======================================================================
# CONFIGURA√á√ÉO DO AGENTE (O NOVO MOTOR V1.0)
# =======================================================================

regras_sistema = """Voc√™ √© um Auditor S√™nior de Contas P√∫blicas. 
Voc√™ tem ferramentas para buscar dados e para salvar relat√≥rios.
REGRA 1: SEMPRE use a ferramenta 'buscar_dados_transparencia' antes de responder.
REGRA 2: Se encontrar anomalias ou gastos excessivos, crie um resumo t√©cnico e SEMPRE use a ferramenta 'guardar_relatorio' para salv√°-lo."""

# üö® Criamos o rob√¥ com a sintaxe super limpa da vers√£o v1.0
auditor_autonomo = create_agent(
    model=llm, 
    tools=ferramentas, 
    system_prompt=regras_sistema
)

# =======================================================================
# EXECU√á√ÉO DO PROGRAMA (O CHAT)
# =======================================================================

print("\nüïµÔ∏è‚Äç‚ôÇÔ∏è Auditor Aut√¥nomo ativo! (Digite 'sair' para encerrar)")

while True:
    pergunta = input("\nVoc√™: ")

    if pergunta.lower() == 'sair': 
        print("Encerrando sistema. At√© logo!")
        break

    print("üßê Analisando e tomando decis√µes...\n")
    
    # O padr√£o do LangChain agora √© receber uma lista de mensagens
    resposta = auditor_autonomo.invoke({"messages": [("user", pergunta)]})

    # A resposta final da IA √© a √∫ltima mensagem do hist√≥rico
    # Pegamos a resposta bruta
    conteudo_bruto = resposta['messages'][-1].content
    
    # Se a API do Gemini devolver uma lista com assinaturas de seguran√ßa, extra√≠mos s√≥ o texto
    if isinstance(conteudo_bruto, list):
        texto_limpo = conteudo_bruto[0].get('text', '')
    else:
        texto_limpo = conteudo_bruto # Se j√° for texto puro, segue o jogo

    print(f"\nü§ñ Auditor: {texto_limpo}")